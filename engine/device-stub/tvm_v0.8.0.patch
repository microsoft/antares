diff --git a/python/tvm/autotvm/task/task.py b/python/tvm/autotvm/task/task.py
index ee1750896..cf1c2433c 100644
--- a/python/tvm/autotvm/task/task.py
+++ b/python/tvm/autotvm/task/task.py
@@ -587,7 +587,7 @@ def compute_flop(sch):
         if isinstance(exp, expr.Call):
             return sum([_count_flop(x) for x in exp.args])
 
-        raise FlopCalculationError("Found unsupported operator in the compute expr")
+        return 1
 
     def traverse(ops):
         """accumulate flops"""
@@ -597,11 +597,10 @@ def compute_flop(sch):
                 num_element = _prod_length(op.axis)
 
                 body = op.body
-                if len(body) != 1:
-                    raise FlopCalculationError("Found multiple output in the compute")
-                exp = body[0]
+                for i in range(len(body)):
+                  exp = body[i]
+                  ret += num_element * _count_flop(exp)
 
-                ret += num_element * _count_flop(exp)
                 ret += traverse([t.op for t in op.input_tensors])
 
             elif isinstance(op, tensor.PlaceholderOp):
@@ -624,6 +623,7 @@ def compute_flop(sch):
             "FLOP for this operator"
         )
 
+    ret = max(ret, int(np.product([int(x) for x in sch.outputs[0].output(0).shape])))
     if ret == 0:
         raise RuntimeError(
             "Cannot find float number operation in this operator. "
diff --git a/python/tvm/autotvm/tuner/tuner.py b/python/tvm/autotvm/tuner/tuner.py
index 8ce6b7445..99587c6ce 100644
--- a/python/tvm/autotvm/tuner/tuner.py
+++ b/python/tvm/autotvm/tuner/tuner.py
@@ -109,7 +109,7 @@ class Tuner(object):
         si_prefix: str
             One of tvm.autotvm.utils.SI_PREFIXES. The SI prefix to use when reporting FLOPS.
         """
-        measure_batch = create_measure_batch(self.task, measure_option)
+        measure_batch = self.measure_batch if hasattr(self, 'measure_batch') else create_measure_batch(self.task, measure_option)
         n_parallel = getattr(measure_batch, "n_parallel", 1)
         early_stopping = early_stopping or 1e9
         self.n_trial = n_trial
diff --git a/src/target/source/codegen_c.cc b/src/target/source/codegen_c.cc
index a31111153..582904b96 100644
--- a/src/target/source/codegen_c.cc
+++ b/src/target/source/codegen_c.cc
@@ -165,7 +165,7 @@ std::string CodeGenC::GetBufferRef(DataType t, const VarNode* buffer, PrimExpr i
   }
   bool is_vol = IsVolatile(buffer);
   if (t.lanes() == 1) {
-    if (!HandleTypeMatch(buffer, t) || is_vol) {
+    if (strncmp(getenv("BACKEND"), "c-hlsl_", 7) && (!HandleTypeMatch(buffer, t) || is_vol)) {
       os << "((";
       if (is_vol) {
         os << "volatile ";
@@ -757,7 +757,42 @@ void CodeGenC::VisitStmt_(const StoreNode* op) {
 
     if (arith::ramp(base, 1, t.lanes()).Match(op->index)) {
       std::string value = this->PrintExpr(op->value);
-      this->PrintVecStore(op->buffer_var.get(), t, base.Eval(), value);
+      if (value.size() >= 5 && value.substr(0, 5) == "make_") {
+        // Expand Vectorize Load/Store to avoid irregular cast.
+        int idx = int(value.find('(')) + 1, stk = 0, nxt;
+        for (int i = 0; i < t.lanes(); ++i) {
+          std::string field;
+          for (nxt = idx; field.size() == 0; ++nxt) {
+            if (value[nxt] == '(' || value[nxt] == '[')
+              ++stk;
+            else if (stk > 0 && (value[nxt] == ')' || value[nxt] == ']'))
+              --stk;
+            else if (!stk && (value[nxt] == ',' || value[nxt] == ')'))
+              field = value.substr(idx, nxt - idx), idx = nxt + 1;
+          }
+          this->PrintIndent();
+          stream << GetVarID(op->buffer_var.get()) << "[(";
+          PrintExpr(base.Eval(), stream);
+          stream << ") + " << i << "] = " << field << ";\n";
+        }
+      } else {
+        auto endsWith = [](const std::string &str, const std::string &end) {
+          return str.size() >= end.size() && str.compare(str.size() - end.size(), end.size(), end) == 0;
+        };
+        std::string vid = GetVarID(op->buffer_var.get());
+        std::string ref = GetBufferRef(t, op->buffer_var.get(), base.Eval());
+        int idx = ref.find(vid);
+        if (idx >= 0 && int(value.size()) >= idx && ref.compare(0, idx, value.c_str(), idx) == 0 && endsWith(ref, ")[0]") && endsWith(value, ")[0]")) {
+          int nxt = value.find(" + ", idx);
+          for (int i = 0; i < t.lanes(); ++i) {
+            this->PrintIndent();
+            stream << "__STORE_ITEM_" << i << "__(" << ref.substr(2, idx - 5) << ", " << vid << ", ";
+            PrintExpr(base.Eval(), stream);
+            stream << ", " << value.substr(idx, nxt - idx) << ", " << value.substr(nxt + 3, value.size() - nxt - 7) << ";\n";
+          }
+        } else
+          this->PrintVecStore(op->buffer_var.get(), t, base.Eval(), value);
+      }
     } else {
       // The assignment below introduces side-effect, and the resulting value cannot
       // be reused across multiple expression, thus a new scope is needed
@@ -881,6 +916,9 @@ void CodeGenC::VisitStmt_(const AttrStmtNode* op) {
     IterVar iv = Downcast<IterVar>(op->node);
     if (iv->thread_tag.length() != 0) {
       if (!var_idmap_.count(iv->var.get())) {
+        int nthread = static_cast<int>(op->value.as<IntImmNode>()->value);
+        if (std::string(iv->thread_tag).find("threadIdx.") == 0 || std::string(iv->thread_tag).find("blockIdx.") == 0)
+          this->stream << "  // [thread_extent] " << iv->thread_tag << " = " << nthread << "\n";
         BindThreadIndex(iv);
       }
     }
diff --git a/src/target/source/codegen_cuda.cc b/src/target/source/codegen_cuda.cc
index a52564c34..ab783fd3b 100644
--- a/src/target/source/codegen_cuda.cc
+++ b/src/target/source/codegen_cuda.cc
@@ -23,6 +23,7 @@
 
 #include "codegen_cuda.h"
 
+#include "../datatype/registry.h"
 #include <tvm/arith/analyzer.h>
 #include <tvm/runtime/registry.h>
 #include <tvm/tir/stmt_functor.h>
@@ -88,6 +89,8 @@ void CodeGenCUDA::PrintExtraAttrs(const PrimFunc& f) {
 }
 
 std::string CodeGenCUDA::Finish() {
+  return CodeGenC::Finish();
+
   if (enable_fp16_) {
     decl_stream << "#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)\n";
     decl_stream << "#include <cuda_fp16.h>\n";
@@ -389,6 +392,9 @@ void CodeGenCUDA::PrintType(DataType t, std::ostream& os) {  // NOLINT(*)
       return;
     }
   }
+
+  auto name = tvm::datatype::Registry::Global()->GetTypeName(t.code());
+  os << name; return;
   LOG(FATAL) << "Cannot convert type " << t << " to CUDA type";
 }
 
@@ -405,23 +411,13 @@ void CodeGenCUDA::PrintVecBinaryOp(const std::string& op, DataType t, PrimExpr l
     std::string vlhs = SSAGetID(PrintExpr(lhs), lhs.dtype());
     std::string vrhs = SSAGetID(PrintExpr(rhs), rhs.dtype());
 
-    for (int i = 0, lanes = t.lanes(); i < lanes; ++i) {
-      std::ostringstream value_temp;
-      if (isalpha(op[0])) {
-        value_temp << op << "(";
-        PrintVecElemLoad(vlhs, lhs.dtype(), i, value_temp);
-        value_temp << ", ";
-        PrintVecElemLoad(vrhs, rhs.dtype(), i, value_temp);
-        value_temp << ")";
-      } else {
-        value_temp << "(";
-        PrintVecElemLoad(vlhs, lhs.dtype(), i, value_temp);
-        value_temp << op;
-        PrintVecElemLoad(vrhs, rhs.dtype(), i, value_temp);
-        value_temp << ")";
-      }
-      PrintVecElemStore(sret, t, i, value_temp.str());
-    }
+    std::ostringstream value_temp;
+    value_temp << "(" << vlhs;
+    value_temp << op;
+    value_temp << vrhs << ")";
+
+    this->PrintIndent();
+    stream << sret << " = " << value_temp.str() << ";\n";
   }
   EndScope(ssa_scope);
   os << sret;
@@ -434,6 +430,9 @@ void CodeGenCUDA::PrintVecElemLoad(const std::string& vec, DataType t, int i,
     return;
   }
 
+  os << "__ITEM_" << i << "_OF__(" << vec << ")";
+  return;
+
   static const char access[] = {'x', 'y', 'z', 'w'};
   ICHECK(i >= 0 && i < (t.bits() == 8 ? 16 : (t.bits() == 16 || t.bits() == 32) ? 8 : 4));
   if (t.bits() == 8 && (t.is_int() || t.is_uint())) {
@@ -475,6 +474,9 @@ void CodeGenCUDA::PrintVecElemLoad(const std::string& vec, DataType t, int i,
 void CodeGenCUDA::PrintVecElemStore(const std::string& vec, DataType t, int i,
                                     const std::string& value) {
   this->PrintIndent();
+  stream << "__ITEM_" << i << "_OF__(" << vec << ") = " << value << ";\n";
+  return;
+
   static const char access[] = {'x', 'y', 'z', 'w'};
   ICHECK(i >= 0 && i < (t.bits() == 8 ? 16 : (t.bits() == 16 || t.bits() == 32) ? 8 : 4));
   if (t.bits() == 8 && (t.is_int() || t.is_uint())) {
diff --git a/src/tir/transforms/arg_binder.cc b/src/tir/transforms/arg_binder.cc
index d3ab32cbd..72b674d9d 100644
--- a/src/tir/transforms/arg_binder.cc
+++ b/src/tir/transforms/arg_binder.cc
@@ -163,7 +163,9 @@ void ArgBinder::BindDLTensor(const Buffer& buffer, const PrimExpr& device_type,
   DataType dtype = buffer->dtype;
   std::ostringstream type_err_msg;
   type_err_msg << arg_name << ".dtype is expected to be " << dtype;
-  PrimExpr cond = (TVMArrayGet(DataType::UInt(8), handle, builtin::kArrTypeCode) ==
+
+  PrimExpr cond = IntImm(DataType::UInt(8), dtype.code()) > IntImm(DataType::UInt(8), DataType::kCustomBegin) ||
+                   (TVMArrayGet(DataType::UInt(8), handle, builtin::kArrTypeCode) ==
                        IntImm(DataType::UInt(8), dtype.code()) &&
                    TVMArrayGet(DataType::UInt(8), handle, builtin::kArrTypeBits) ==
                        IntImm(DataType::UInt(8), dtype.bits()) &&
diff --git a/src/tir/transforms/lower_thread_allreduce.cc b/src/tir/transforms/lower_thread_allreduce.cc
index 6f7c09cdc..30af75707 100644
--- a/src/tir/transforms/lower_thread_allreduce.cc
+++ b/src/tir/transforms/lower_thread_allreduce.cc
@@ -29,6 +29,7 @@
 #include <tvm/tir/stmt_functor.h>
 #include <tvm/tir/transform.h>
 
+#include <cuda.h>
 #include <unordered_set>
 
 #include "../../runtime/thread_storage_scope.h"
@@ -62,7 +63,7 @@ class UpdatePointerStorageScopeAllReduce final : public UpdatePointerStorageScop
 class ThreadAllreduceBuilder final : public StmtExprMutator {
  public:
   explicit ThreadAllreduceBuilder(const TargetNode* target)
-      : target_(target), warp_size_(target->GetAttr<Integer>("thread_warp_size", 1).value()) {}
+      : target_(target), warp_size_(target->GetAttr<Integer>("thread_warp_size", 1).value()) { cudaDeviceGetAttribute(&warp_size_, cudaDevAttrWarpSize, 0); }
 
   Stmt VisitStmt_(const AttrStmtNode* op) final {
     if (op->attr_key == attr::thread_extent) {
@@ -551,7 +552,7 @@ class ThreadAllreduceBuilder final : public StmtExprMutator {
   // Also, the warp/wavefront size differs (64 on rocm, 32 on cuda).
   bool is_warp_reduction(const std::vector<DataType>& types) const {
     // Only cuda target supports warp reductions.
-    if ((target_->kind->name != "cuda") && (target_->kind->name != "rocm")) return false;
+    if (strncmp("c-cuda", getenv("BACKEND"), 6) != 0) return false;
 
     // rocm only supports 32 bit operands for shuffling at the moment
     if ((target_->kind->name == "rocm") &&
diff --git a/src/tir/transforms/split_host_device.cc b/src/tir/transforms/split_host_device.cc
index 7f2ecf54d..74f26e233 100644
--- a/src/tir/transforms/split_host_device.cc
+++ b/src/tir/transforms/split_host_device.cc
@@ -107,6 +107,7 @@ class VarUseDefAnalysis : public StmtExprMutator {
   }
 
   Stmt VisitStmt_(const StoreNode* op) final {
+    this->output_hints.insert(op->buffer_var.get()->name_hint);
     this->HandleUse(op->buffer_var);
     return StmtExprMutator::VisitStmt_(op);
   }
@@ -193,6 +194,7 @@ class VarUseDefAnalysis : public StmtExprMutator {
   bool use_dyn_shmem_{false};
   std::unordered_map<const VarNode*, int> use_count_;
   std::unordered_map<const VarNode*, int> def_count_;
+  std::unordered_set<std::string> output_hints;
 
  private:
   ExprDeepEqual deep_equal_;
@@ -249,7 +251,15 @@ class HostDeviceSplitter : public StmtMutator {
     Map<tir::Var, PrimExpr> remap_vars;
 
     // Strictly order the arguments: Var pointers, positional arguments.
-    for (Var var : m.undefined_) {
+    std::vector<Var> ordered_args(m.undefined_.begin(), m.undefined_.end());
+    std::sort(ordered_args.begin(), ordered_args.end(), [&](const Var &x, const Var &y) {
+      int x_access = m.output_hints.count(x.get()->name_hint);
+      int y_access = m.output_hints.count(y.get()->name_hint);
+      if (x_access != y_access)
+        return x_access < y_access;
+      return x.get()->name_hint < y.get()->name_hint;
+    });
+    for (Var var : ordered_args) {
       if (var.dtype().is_handle()) {
         // Create a new version of v.
         auto it = handle_data_type_.find(var.get());
diff --git a/src/tir/transforms/storage_rewrite.cc b/src/tir/transforms/storage_rewrite.cc
index 409b7c262..37d0077dc 100644
--- a/src/tir/transforms/storage_rewrite.cc
+++ b/src/tir/transforms/storage_rewrite.cc
@@ -341,6 +341,8 @@ class StoragePlanRewriter : public StmtExprMutator {
   using AllocEntry = LinearAccessPatternFinder::AllocEntry;
 
   Stmt Rewrite(Stmt stmt, bool detect_inplace) {
+    return stmt;
+
     detect_inplace_ = detect_inplace;
     // plan the rewrite
     LinearAccessPatternFinder finder;
